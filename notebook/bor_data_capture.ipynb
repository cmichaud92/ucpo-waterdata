{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a15897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from typing import Optional\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.duckdb_utils import connect_duckdb\n",
    "from utils.duckdb_utils import run_sql_file\n",
    "import utils.site_list as sl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fde0100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['917', '919', '913', '914', '915', '928', '1999', '2000', '2002', '2005']\n",
      "['FLAMING GORGE RESERVOIR', 'LAKE POWELL', 'BLUE MESA RESERVOIR', 'MORROW POINT RESERVOIR', 'CRYSTAL RESERVOIR', 'STARVATION RESERVOIR', 'GRANBY RESERVOIR', 'GREEN MOUNTAIN RESERVOIR', 'RUEDI RESERVOIR', 'WILLIAMS FORK RESERVOIR']\n"
     ]
    }
   ],
   "source": [
    "with connect_duckdb() as con:\n",
    "    result = con.execute(\"SELECT site_cd, site_nm FROM site WHERE agency_cd = 'BOR'\").fetchall()\n",
    "    sites = [row[0] for row in result]\n",
    "    site_names = [row[1] for row in result]\n",
    "\n",
    "parameters = [17, 29, 42, 49]\n",
    "base_url = \"https://usbr.gov/uc/water/hydrodata/reservoir_data/\"\n",
    "print(sites)\n",
    "print(site_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5db3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_duckdb() as con:\n",
    "    result = con.execute(\"SELECT * FROM parameter\").df()\n",
    "    result.to_csv(\"parameters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d14dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "UPDATE parameter\n",
    "SET parameter_dsc = 'Pool elevation'\n",
    "WHERE parameter_cd = 49\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_duckdb() as con:\n",
    "    result = con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60241878",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = sites[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in sites:\n",
    "    all_data = []\n",
    "    for pcode in parameters:\n",
    "        url = f\"{base_url}{site}/csv/{pcode}.csv\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                df = pd.read_csv(url, header=0, names=['date', 'value'], parse_dates=['date'])\n",
    "                df['site_cd'] = site\n",
    "                df['parameter_cd'] = str(pcode)\n",
    "                if df.empty:\n",
    "                    print(f\"No data found for site {site} with parameter {pcode}.\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for site {site} with parameter {pcode}: HTTP {response.status_code}\")\n",
    "                continue\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for site {site} with parameter {pcode}: {e}\")\n",
    "            continue\n",
    "        if not df.empty:\n",
    "            all_data.append(df)\n",
    "    if all_data:\n",
    "        df_combined = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# https://usbr.gov/uc/water/hydrodata/reservoir_data/100010/csv/17.csv\n",
    "# https://usbr.gov/uc/water/hydrodata/reservoir_data/100010/csv/49.csv\n",
    "# https://usbr.gov/uc/water/hydrodata/reservoir_data/100089\n",
    "# https://usbr.gov/uc/water/hydrodata/reservoir_data/917/csv/17.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca0e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'site_id': 'site_cd', 'datatype_id': 'parameter_cd', 'site_metadata.site_name': 'site_nm', \n",
    "    'datatype_metadata.datatype_common_name': 'parameter_nm', \n",
    "    'datatype_metadata.physical_quantity_name': 'parameter_type', \n",
    "    'datatype_metadata.unit_name': 'units', 'site_metadata.lat': 'latitude_dd',\n",
    "    'site_metadata.longi': 'longitude_dd', 'site_metadata.elevation': 'elevation_m',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa992bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern: Gramby|Green\\ Mountain|Ruedi|Williams\\ Fork|Willow\\ Creek|Windy\\ Gap|Wolford|Flaming\\ Gorge|Granby|Green\\ Mountain|Ruedi|Williams\\ Fork|Willow\\ Creek|Windy\\ Gap|Wolford\\ Mountain|Flaming\\ Gorge|Starvation|Catamount|Stagecoach|Shadow\\ Mountain|Blue\\ Mesa|Crystal|Morrow\\ Point|Ridgeway|Powell\n"
     ]
    }
   ],
   "source": [
    "names = [\n",
    "    \"Gramby\", \"Green Mountain\", \"Ruedi\", \"Williams Fork\", \"Willow Creek\", \n",
    "    \"Windy Gap\", \"Wolford\", \"Flaming Gorge\", \"Granby\", \"Green Mountain\", \n",
    "    \"Ruedi\", \"Williams Fork\", \"Willow Creek\", \"Windy Gap\", \"Wolford Mountain\", \n",
    "    \"Flaming Gorge\", \"Starvation\", \"Catamount\", \"Stagecoach\", \"Shadow Mountain\",\n",
    "    \"Blue Mesa\", \"Crystal\", \"Morrow Point\", \"Ridgeway\",\n",
    "    \"Powell\" \n",
    "    ]\n",
    "\n",
    "parameters = [17, 29, 42, 49] \n",
    "pattern = \"|\".join(re.escape(name) for name in names)\n",
    "\n",
    "print(\"Pattern:\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c5f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15', '47', '25', '29', '30', '31', '32', '33', '34', '1197', '1198', '39', '40', '43', '46', '42', '17', '89', '49']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://www.usbr.gov/uc/water/hydrodata/reservoir_data/meta.csv\")\n",
    "\n",
    "matches = df[(df[\"site_metadata.site_name\"].str.contains(pattern, case = False, na=False))].sort_values(\"site_metadata.site_name\")# &\n",
    "  #           (df[\"site_metadata.db_site_code\"] == \"UC\")].sort_values(\"site_metadata.site_name\") #&\n",
    "            # (df[\"datatype_id\"].isin(parameters))]\n",
    "             \n",
    "\n",
    "selected = matches[list(rename_map)].rename(columns=rename_map).reset_index(drop=True)\n",
    "selected['parameter_cd'] = selected['parameter_cd'].astype(str)\n",
    "selected['site_cd'] = selected['site_cd'].astype(str)\n",
    "#ck2 = selected[list['site_cd', 'site_nm', 'latitude_dd', 'longitude_dd', 'elevation_m']]\n",
    "sites = selected.drop_duplicates(subset=['site_cd']).reset_index(drop=True)\n",
    "\n",
    "ck = selected[list(['site_cd', 'site_nm', 'parameter_cd', 'parameter_nm', 'parameter_type', 'units'])]\n",
    "all_params = ck[list(['parameter_cd', 'parameter_nm', 'parameter_type', 'units'])].drop_duplicates().reset_index(drop=True).sort_values(['parameter_type', 'parameter_nm'])\n",
    "#print(ck)\n",
    "all_params.to_csv(\"usbr_parameters.csv\", index=False)\n",
    "params_list = all_params['parameter_cd'].astype(str).tolist()\n",
    "#selected[list(['site_cd', 'site_nm'])].drop_duplicates().to_csv(\"usbr_sites.csv\", index=False)\n",
    "\n",
    "print(params_list)\n",
    "\n",
    "with connect_duckdb() as con:\n",
    "    site_tbl = con.execute(\"SELECT site_id, site_cd FROM site\").df()\n",
    "    param_tbl = con.execute(\"SELECT parameter_id, parameter_cd FROM parameter\").df()\n",
    "\n",
    "site_param = selected[\n",
    "    list(['site_cd', 'parameter_cd'])\n",
    "    ].merge(site_tbl, on='site_cd', how='left').merge(param_tbl, on='parameter_cd', how='left').drop_duplicates().reset_index(drop=True)\n",
    "site_param['site_parameter_id'] = range(1, len(site_param) + 1)\n",
    "site_param = site_param[list(['site_parameter_id', 'site_id', 'parameter_id'])]\n",
    "site_param.to_csv(\"usbr_site_parameter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33c01147",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_duckdb() as con:\n",
    "    con.execute(\"DROP TABLE IF EXISTS site_parameter;\")\n",
    "\n",
    "run_sql_file(Path('../db/schema.sql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fe9ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "usbr_site_param = pd.read_csv(\"../artifacts/usbr_site_parameter.csv\")\n",
    "usbr_site_param['created_ts'] = pd.Timestamp.now()\n",
    "usbr_site_param['modified_ts'] = pd.Timestamp.now()\n",
    "\n",
    "with connect_duckdb() as con:\n",
    "    con.register('site_param', usbr_site_param)\n",
    "    con.execute(\"INSERT INTO site_parameter SELECT * FROM site_param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c26778ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_duckdb() as con:\n",
    "    result = con.execute(\"\"\"\n",
    "                         SELECT \n",
    "                            s.site_cd, s.site_nm, s.site_type, p.parameter_cd, p.parameter_nm, p.unit_nm, p.unit_cd \n",
    "                         FROM site s\n",
    "                         INNER JOIN site_parameter sp ON s.site_id = sp.site_id\n",
    "                         INNER JOIN parameter p ON sp.parameter_id = p.parameter_id\n",
    "                         WHERE s.agency_cd = 'BOR'\n",
    "                         \"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfed2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sl.params\n",
    "params['parameter_id'] = range(1, len(params) +1)\n",
    "params['create_ts'] = pd.Timestamp.now()\n",
    "params['update_ts'] = pd.Timestamp.now()\n",
    "selected_params = params[list(['parameter_id', 'parameter_cd', 'parameter_nm', 'parameter_dsc', 'unit_cd', 'unit_nm', 'create_ts', 'update_ts'])]\n",
    "selected_params = params[list(['parameter_id', 'parameter_cd', ])]\n",
    "selected=selected[list(['site_cd', 'parameter_cd'])]\n",
    "\n",
    "selected = selected.merge(selected_params, on='parameter_cd', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63658dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_duckdb() as con:\n",
    "    site = con.execute(\"SELECT site_id, site_cd FROM SITE\").df()\n",
    "\n",
    "site_param = selected.merge(site, on='site_cd', how='inner').drop_duplicates(subset=['site_cd', 'parameter_cd']).reset_index(drop=True)\n",
    "site_param['site_parameter_id'] = range(1, len(site_param) + 1)\n",
    "site_param['create_ts'] = pd.Timestamp.now()\n",
    "site_param['update_ts'] = pd.Timestamp.now()\n",
    "site_param = site_param[list(['site_parameter_id', 'site_id', 'parameter_id', 'create_ts', 'update_ts'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceef17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO parameter SELECT * FROM staging_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05739198",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_duckdb() as con:\n",
    "    try:\n",
    "        con.register('staging_table', selected)\n",
    "        con.execute(query)\n",
    "    except Exception as e:\n",
    "            print(f\"âŒ Error executing SQL: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6978f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e162aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do not think RISE API is currently available, so this function is a placeholder.\n",
    "def fetch_rise_timeseries(\n",
    "        site_cd: str,\n",
    "        parameter_cd: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        observed_modeled: str = \"observed\",\n",
    "        base_url: str = \"https://data.usbr.gov/rise/api/timeseries\",\n",
    "        format: str = \"json\"\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetches time series data from the RISE API for a given site and parameter.\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"locationId\": site_cd,\n",
    "        \"parameterId\": parameter_cd,\n",
    "        \"startDate\": start_date,\n",
    "        \"endDate\": end_date,\n",
    "        \"observedModeled\": observed_modeled,\n",
    "        \"format\": format\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.api+json\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "        if format == \"json\":\n",
    "            json_data = response.json()\n",
    "            records = json_data.get(\"timeSeries\", [])\n",
    "            if not records:\n",
    "                print(f\"No data found for site {site_cd} and parameter {parameter_cd}.\")\n",
    "                return None\n",
    "            \n",
    "            df = pd.DataFrame(records)\n",
    "            return df\n",
    "        \n",
    "        elif format == \"csv\":\n",
    "            from io import StringIO\n",
    "            return pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported format. Use 'json' or 'csv'.\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data for site {site_cd} and parameter {parameter_cd}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_rise_timeseries(\n",
    "    site_cd='2002',     # Example: Ruedi Reservoir\n",
    "    parameter_cd='29',      # Example: Storage\n",
    "    start_date=\"2024-10-01\",\n",
    "    end_date=\"2024-12-31\"\n",
    ")\n",
    "\n",
    "if df is not None:\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8256e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.cbrfc.noaa.gov/wsup/graph/espgraph_hc.html?year=2025&id=CAMC2#\n",
    "https://www.cbrfc.noaa.gov/wsup/graph/espgraph_hc.html?year=2025&id=CAMC2#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucpo_waterdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
