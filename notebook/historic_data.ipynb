{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9405be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataretrieval.nwis as nwis\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7ff04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_sites = [\n",
    "    \"09152500\",   # Gunnison River Near Grand Junction, CO\n",
    "    \"09095500\",   # Colorado River Near Cameo, CO\n",
    "    \"09106150\",   # Colorado River Below Grand Valley Div NR Palisade, CO\n",
    "    \"09106485\",   # Colorado River Above Gunnison River at Grand Junction, CO\n",
    "    \"09163500\",   # Colorado River Near Colorado-utah State Line\n",
    "    \"09306500\",   # White River Near Watson, Utah\n",
    "    \"09251000\",   # Yampa River Near Maybell, CO\n",
    "    \"09260050\",   # Yampa River at Deerlodge Park, CO\n",
    "    \"09260000\",   # Little Snake River Near Lily, CO\n",
    "    \"09261000\",   # Green River Near Jensen, UT\n",
    "    \"09315000\",   # Green River at Green River, UT\n",
    "    \"09302000\",   # Duchesne River Near Randlett, UT\n",
    "    \"09180000\",   # Dolores River Near Cisco, UT\n",
    "    \"09328960\",   # Colorado River at Gypsum Canyon Near Hite, UT\n",
    "    \"09147022\",   # Ridgeway Reservoir Near Ridgway, CO\n",
    "    \"09041395\",   # Wolford Mtn Reservoir Nr Kremmling, CO\n",
    "    \"09379900\",   # Lake Powell at Glen Canyon Dam, AZ\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(site: str, pcode: str, start_date: str, end_date: str, service_code: str = 'iv') -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch data from NWIS for a given site and parameter code.\n",
    "    Logs an error if the request fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = nwis.get_record(\n",
    "            sites=site,\n",
    "            service=service_code,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            parameterCd=pcode\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching data for site {site} and parameter {pcode}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.warning(f\"No data returned for site {site} and parameter {pcode}.\")\n",
    "        return None\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: pd.DataFrame, site: str, pcode: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform raw NWIS 'iv' data into standardized long format.\n",
    "    \n",
    "    Parameters:\n",
    "        df: Raw dataframe from nwis.get_record()\n",
    "        site: USGS site number\n",
    "        pcode: Parameter code (e.g., '00060' for discharge)\n",
    "    Returns:\n",
    "        A cleaned DataFrame with standard columns:\n",
    "        ['site', 'datetime', 'parameter', 'value', 'approval_status', 'year']\n",
    "    \"\"\"\n",
    "\n",
    "    # Set fail-safe defaults incase no data is available\n",
    "    if df is None or df.empty:\n",
    "        logging.warning(f\"No data to transform for site {site} and parameter {pcode}.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Datetime is the index in the raw data\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Ensure datetime column is present\n",
    "    if 'datetime' not in df.columns:\n",
    "        logging.error(f\"Missing 'datetime' column in data for site {site} and parameter {pcode}.\")\n",
    "        raise ValueError(\"Missing 'datetime' column.\")\n",
    "    \n",
    "    # Identify value column and approval status column\n",
    "    value_cols = [col for col in df.columns if col not in ('site_no', 'datetime') and not col.endswith('cd')]\n",
    "    code_cols = [col for col in df.columns if col.endswith('cd')]\n",
    "\n",
    "    if len(value_cols) != 1:\n",
    "        logging.error(f\"Expected exactly one value column for site {site} and parameter {pcode}. Found: {value_cols}\")\n",
    "        raise ValueError(\"Expected exactly one value column.\")\n",
    "    if len(code_cols) != 1:\n",
    "        logging.error(f\"Expected exactly one code column for site {site} and parameter {pcode}. Found: {code_cols}\")\n",
    "        raise ValueError(\"Expected exactly one code column.\")\n",
    "    \n",
    "    value_col = value_cols[0]\n",
    "    code_col = code_cols[0]\n",
    "\n",
    "    # Construct clean output\n",
    "    df_clean = pd.DataFrame({\n",
    "        'site': site,\n",
    "        'datetime': pd.to_datetime(df['datetime']),\n",
    "        'parameter': pcode,\n",
    "        'value': pd.to_numeric(df[value_col], errors='coerce'),\n",
    "        'approval_status': df[code_col].str[0], # Assuming first character is the status (P or A)\n",
    "    })\n",
    "\n",
    "    # Derived field for partitioning\n",
    "    df_clean['year'] = df_clean['datetime'].dt.year\n",
    "    \n",
    "    # Drop bad rows (e.g., NaN values in 'site', 'value' or 'datetime')\n",
    "    required_fields = ['site', 'datetime', 'value']\n",
    "    df_clean = df_clean.dropna(subset=required_fields)\n",
    "    \n",
    "    return df_clean\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_datalake(df: pd.DataFrame, site: str, output_root: str) -> None:\n",
    "    \"\"\"\n",
    "    Write transformed USGS IV data to partitioned parquet files in the datalake.\n",
    "\n",
    "    Parameters: \n",
    "        df              : Transformed DataFrame with columns including ['site', 'datetime', 'parameter', 'value', 'approval_status', 'year']\n",
    "        site            : USGS site number\n",
    "        output_root     : Root directory for the datalake\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.warning(f\"No data to write for site {site}.\")\n",
    "        return\n",
    "    \n",
    "    # Sort data by datetime for performance and compression\n",
    "    df_sorted = df.sort_values(by='datetime')\n",
    "\n",
    "    # Partition by year and site and write to parquet\n",
    "    for year, group in df_sorted.groupby('year'):\n",
    "        output_path = Path(output_root) / \"timeseries_iv\" / f\"site={site}\" / f\"year={year}\"\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        file_path = output_path / \"data.parquet\"\n",
    "        try:\n",
    "            group = group.copy()\n",
    "\n",
    "            # Strip timezone from datetime if present\n",
    "            if isinstance(group[\"datetime\"].dtype, pd.DatetimeTZDtype):\n",
    "                group[\"datetime\"] = group[\"datetime\"].dt.tz_localize(None)\n",
    "\n",
    "            # Ensure correct data types\n",
    "            group = group.astype({\n",
    "                \"site\": \"string\",\n",
    "                \"datetime\": \"datetime64[ns]\",\n",
    "                \"parameter\": \"string\",\n",
    "                \"value\": \"float64\",\n",
    "                \"approval_status\": \"string\",\n",
    "                \"year\": \"int64\"\n",
    "            })\n",
    "\n",
    "            duckdb.register(\"temp_df\", group)\n",
    "            duckdb.sql(f\"COPY temp_df TO '{file_path}' (FORMAT PARQUET)\")\n",
    "            duckdb.unregister(\"temp_df\")\n",
    "            logging.info(f\"{len(group)} rows → {file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing data for site {site} and year {year}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWIS Data Capture for Streamflow and Water Quality - \"Instantaneous Values\"\n",
    "# This script fetches data from the USGS NWIS database for specified sites and parameter codes.\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "sites = usgs_sites\n",
    "parameter_codes = ['00060', '00010', '62614']\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2025-01-01'\n",
    "service_code = 'iv'\n",
    "output_root = 'data/data_lake'\n",
    "# ======================\n",
    "\n",
    "\n",
    "# Configure logging ------------------------------------------------\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "log_name = 'logs/' + datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.log'\n",
    "logging.basicConfig(filename=log_name,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Itterate through each site\n",
    "for site in sites:\n",
    "    print(f\"Processing site: {site}\")\n",
    "    all_data = []\n",
    "\n",
    "    # Itterate through each parameter code\n",
    "    for pcode in parameter_codes:\n",
    "        \n",
    "        # Fetch data for the current site and parameter code from NWIS\n",
    "        df_raw = fetch_data(site, pcode, start_date, end_date, service_code)\n",
    "        if df_raw is None:\n",
    "            continue\n",
    "\n",
    "        # Clean and transform the data, standardizing column names and types\n",
    "        df_transformed = transform_data(df_raw, site, pcode)\n",
    "        if df_transformed.empty:\n",
    "            continue\n",
    "\n",
    "        # Write the data to a parquet file\n",
    "        write_to_datalake(df_transformed, site, output_root)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM 'data/data_lake/timeseries_iv/site=*/year=*/data.parquet'\n",
    "WHERE site = '09379900'\n",
    "  AND parameter = '62614'\n",
    "  AND datetime BETWEEN TIMESTAMP '2024-01-01' AND TIMESTAMP '2024-01-15'\n",
    "  AND approval_status = 'A'\n",
    "ORDER BY datetime\n",
    "\"\"\"\n",
    "\n",
    "df = duckdb.sql(query).df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c455c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Multipurpose function to capture gage data\n",
    "def unit_value_fetch_and_store_by_site_year(\n",
    "        sites: list, \n",
    "        start_date: str, \n",
    "        end_date: str, \n",
    "        parameter_codes: list,\n",
    "        service_code: str = \"iv\", \n",
    "        output_root: Path = \"data/data_lake\"\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Fetch unit value (iv) data from NWIS for one or more gaging stations.\n",
    "    Will also capture daily value optionally.\n",
    "    Chunk the data by site and year and write to partitioned parquet files\n",
    "    \"\"\"\n",
    "    assert service_code in (\"iv\", \"dv\"), \"Service code must be 'iv' or 'dv'\"\n",
    "    output_root = Path(output_root)\n",
    "    \n",
    "    # Set top level folder based on service type\n",
    "    if service_code == \"iv\":\n",
    "        table_dir = \"timeseries_iv\"\n",
    "    else:\n",
    "        table_dir = \"daily_values\"\n",
    "\n",
    "    \n",
    "    for site in sites:\n",
    "        print(f\"Processing site: {site}\")\n",
    "        all_data = []\n",
    "\n",
    "        for pcode in parameter_codes:\n",
    "            try:\n",
    "                df = nwis.get_record(\n",
    "                    sites=site,\n",
    "                    service=service_code,\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    parameterCd=pcode\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {site}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data returned for {site}\")\n",
    "                continue\n",
    "\n",
    "            df = df.reset_index()\n",
    "            time_column = \"datetime\" if \"datetime\" in df.columns else \"date\"\n",
    "            df['parameter'] = pcode\n",
    "            df['value'] = df[df.columns.difference(['site_no', time_column, 'parameter'])[0]]\n",
    "            df = df[['site_no', time_column, 'parameter', 'value']]\n",
    "            df = df.rename(columns={'site_no': 'site', time_column: 'datetime'})\n",
    "            df['year'] = df['datetime'].dt.year\n",
    "\n",
    "            all_data.append(df)\n",
    "\n",
    "        if not all_data:\n",
    "            continue\n",
    "        \n",
    "        df_all = pd.concat(all_data)\n",
    "        df_all_sorted = df_all.sort_values(by=['datetime', 'parameter'])\n",
    "        \n",
    "        for year, group in df_all_sorted.groupby('year'):\n",
    "            output_path = output_root / table_dir / f\"site={site}\" / f\"year={year}\"\n",
    "            output_path.mkdir(parents=True, exist_ok=True)\n",
    "            file = output_path / \"data.parquet\"\n",
    "\n",
    "            # ✅ Ensure consistent data types\n",
    "            group['site'] = group['site'].astype(str)\n",
    "            \n",
    "            try:\n",
    "                # Write with duckdb to ensure consistent types\n",
    "                # duckdb.sql(\"CREATE OR REPLACE TABLE temp AS SELECT * FROM group\")\n",
    "                duckdb.register(\"temp_df\", group)\n",
    "                duckdb.sql(f\"COPY temp_df TO '{file}' (FORMAT PARQUET)\")\n",
    "                duckdb.unregister(\"temp_df\")\n",
    "                print(f\"   Wrote {len(group)} records to {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write {file}: {e}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b859e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[usgs_sites[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07214948",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in [usgs_sites[0]]:\n",
    "        print(f\"Processing site: {site}\")\n",
    "        all_data = []\n",
    "\n",
    "        for pcode in ['00060', '00010']:\n",
    "            try:\n",
    "                df = nwis.get_record(\n",
    "                    sites=site,\n",
    "                    service='iv',\n",
    "                    start=\"2024-01-01\",\n",
    "                    end=\"2025-12-31\",\n",
    "                    parameterCd=pcode\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {site}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data returned for {site}\")\n",
    "                continue\n",
    "\n",
    "            df = df.reset_index()\n",
    "            time_column = \"datetime\" if \"datetime\" in df.columns else \"date\"\n",
    "            df['parameter'] = pcode\n",
    "            df['value'] = df[df.columns.difference(['site_no', time_column, 'parameter'])[0]]\n",
    "            df = df[['site_no', time_column, 'parameter', 'value']]\n",
    "            df = df.rename(columns={'site_no': 'site', time_column: 'datetime'})\n",
    "            df['year'] = df['datetime'].dt.year\n",
    "\n",
    "            all_data.append(df)\n",
    "\n",
    "        if not all_data:\n",
    "            continue\n",
    "        \n",
    "        df_all = pd.concat(all_data)\n",
    "        df_all_sorted = df_all.sort_values(by=['datetime', 'parameter'])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_value_fetch_and_store_by_site_year(sites=[usgs_sites[0]], service_code=\"iv\", start_date=\"2024-01-01\", end_date=\"2025-12-31\", parameter_codes=[\"00060\", \"00010\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d562dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.read_parquet('data/data_lake/timeseries_iv/site=09180000/year=2024/data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dede1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table('data/data_lake/timeseries_iv/site=09152500/year=2024/data.parquet')\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = usgs_gage_data(sites=usgs_sites, service=\"dv\", start=\"2000-01-01\", end=\"2005-01-01\", parameterCd=\"00060\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis.get_record(\n",
    "    sites=usgs_sites[1],\n",
    "    service=\"dv\",\n",
    "    start=\"2025-01-01\",\n",
    "    end=\"2025-05-14\",\n",
    "    parameterCd='00060'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in usgs_sites:\n",
    "    print(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis.get_record(sites=['03339000', '09180000'], service='dv', start='2017-12-31', parameterCd='00060')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect(\"waterdata_lakehouse.duckdb\") as conn:\n",
    "    conn.register(\"clean_sites\", clean_sites)\n",
    "    conn.execute(\"INSERT OR REPLACE INTO site SELECT * FROM clean_sites\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
