{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893ab2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import logging\n",
    "import waterdata_utils as wdu\n",
    "import duckdb\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b84b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_sites = [\n",
    "    \"09152500\",   # Gunnison River Near Grand Junction, CO\n",
    "    \"09095500\",   # Colorado River Near Cameo, CO\n",
    "    \"09106150\",   # Colorado River Below Grand Valley Div NR Palisade, CO\n",
    "    \"09163500\",   # Colorado River Near Colorado-utah State Line\n",
    "    \"09306500\",   # White River Near Watson, Utah\n",
    "    \"09251000\",   # Yampa River Near Maybell, CO\n",
    "    \"09260050\",   # Yampa River at Deerlodge Park, CO\n",
    "    \"09260000\",   # Little Snake River Near Lily, CO\n",
    "    \"09261000\",   # Green River Near Jensen, UT\n",
    "#    \"09315000\",   # Green River at Green River, UT\n",
    "    \"09302000\",   # Duchesne River Near Randlett, UT\n",
    "    \"09180000\",   # Dolores River Near Cisco, UT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c43794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "sites = usgs_sites\n",
    "parameter_codes = ['00060', '00010']\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2025-03-01'\n",
    "service_code = 'iv'\n",
    "output_root = '/Volumes/T7_raw_I/waterdata_lake'\n",
    "# ======================\n",
    "\n",
    "\n",
    "# Configure logging ------------------------------------------------\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "log_name = 'logs/' + dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.log'\n",
    "logging.basicConfig(filename=log_name,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e66295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: 09152500\n",
      "Processing site: 09095500\n",
      "Processing site: 09106150\n",
      "Processing site: 09163500\n",
      "Processing site: 09306500\n",
      "Processing site: 09251000\n",
      "Processing site: 09260050\n",
      "Processing site: 09260000\n",
      "Processing site: 09261000\n",
      "Processing site: 09302000\n",
      "Processing site: 09180000\n"
     ]
    }
   ],
   "source": [
    "# Itterate through each site main function\n",
    "for site in sites:\n",
    "    print(f\"Processing site: {site}\")\n",
    "    all_data = []\n",
    "\n",
    "    # Itterate through each parameter code\n",
    "    for pcode in parameter_codes:\n",
    "        \n",
    "        # Fetch data for the current site and parameter code from NWIS\n",
    "        df_raw = wdu.fetch_data(site, pcode, start_date, end_date, service_code)\n",
    "        if df_raw is None:\n",
    "            continue\n",
    "\n",
    "        # Clean and transform the data, standardizing column names and types\n",
    "        df_transformed = wdu.transform_data(df_raw, site, pcode)\n",
    "        if df_transformed.empty:\n",
    "            continue\n",
    "\n",
    "        # Write the data to a parquet file\n",
    "        wdu.write_to_datalake(df_transformed, site, output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e164feb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT date_trunc('day', datetime) AS day, AVG(value) AS avg_flow FROM read_parquet('/Volumes/T7_raw_I/waterdata_lake/timeseries_iv/**/*.parquet') WHERE site = '09180000' AND parameter = '00060' GROUP BY day ORDER BY day;\n"
     ]
    }
   ],
   "source": [
    "query = (\"\"\"\n",
    "SELECT *\n",
    "FROM read_parquet('/Volumes/T7_raw_I/waterdata_lake/timeseries_iv/**/*.parquet')\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    f\"FROM '{output_root}/timeseries_vi/' \"\n",
    "    \"WHERE site = '09180000' AND year = 2023\"\n",
    ")\n",
    "query = (\n",
    "    \"SELECT date_trunc('day', datetime) AS day, \"\n",
    "    \"AVG(value) AS avg_flow \"\n",
    "    \"FROM read_parquet('/Volumes/T7_raw_I/waterdata_lake/timeseries_iv/**/*.parquet') \"\n",
    "    \"WHERE site = '09180000' \"\n",
    "    \"AND parameter = '00060' \"  # Replace with actual parameter code if needed\"\n",
    "    \"GROUP BY day \"\n",
    "    \"ORDER BY day;\"\n",
    ")\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d73f2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [day, avg_flow]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to DuckDB and execute the query\n",
    "with duckdb.connect() as con:\n",
    "    result = con.execute(query).fetchdf()\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afdd3fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: No files found that match the pattern \"/volumes/T7_raw_I/waterdata_lake/timeseries_iv/\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckdb\u001b[39;00m\n\u001b[32m      3\u001b[39m con = duckdb.connect()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    SELECT *\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m    FROM read_parquet(\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/volumes/T7_raw_I/waterdata_lake/timeseries_iv/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m)\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m    WHERE site = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m09180000\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AND year = 2023\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m.fetchdf()\n",
      "\u001b[31mIOException\u001b[39m: IO Error: No files found that match the pattern \"/volumes/T7_raw_I/waterdata_lake/timeseries_iv/\""
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('/volumes/T7_raw_I/waterdata_lake/timeseries_iv/')\n",
    "    WHERE site = '09180000' AND year = 2023\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61de679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e1c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.read_parquet(\n",
    "    f\"{output_root}/timeseries_iv/site=09180000/year=2023/data.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c0745e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────┬─────────────────────┬───────────┬────────┬─────────────────┬───────┐\n",
      "│   site   │      datetime       │ parameter │ value  │ approval_status │ year  │\n",
      "│ varchar  │    timestamp_ns     │  varchar  │ double │     varchar     │ int64 │\n",
      "├──────────┼─────────────────────┼───────────┼────────┼─────────────────┼───────┤\n",
      "│ 09180000 │ 2023-01-01 00:00:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 00:15:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 00:30:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 00:45:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 01:00:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 01:15:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 01:30:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 01:45:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 02:00:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-01-01 02:15:00 │ 00010     │    2.8 │ A               │  2023 │\n",
      "│    ·     │          ·          │   ·       │     ·  │ ·               │    ·  │\n",
      "│    ·     │          ·          │   ·       │     ·  │ ·               │    ·  │\n",
      "│    ·     │          ·          │   ·       │     ·  │ ·               │    ·  │\n",
      "│ 09180000 │ 2023-04-15 04:15:00 │ 00010     │    7.2 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 04:30:00 │ 00010     │    7.2 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 04:45:00 │ 00010     │    7.1 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 05:00:00 │ 00010     │    7.0 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 05:15:00 │ 00010     │    7.0 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 05:30:00 │ 00010     │    7.0 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 05:45:00 │ 00010     │    6.9 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 06:00:00 │ 00010     │    6.9 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 06:15:00 │ 00010     │    6.8 │ A               │  2023 │\n",
      "│ 09180000 │ 2023-04-15 06:30:00 │ 00010     │    6.8 │ A               │  2023 │\n",
      "├──────────┴─────────────────────┴───────────┴────────┴─────────────────┴───────┤\n",
      "│ ? rows (>9999 rows, 20 shown)                                       6 columns │\n",
      "└───────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a623c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table('data/data_lake/timeseries_iv/site=09152500/year=2022/data.parquet')\n",
    "df = table.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
