{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "893ab2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import logging\n",
    "import waterdata_utils as wdu\n",
    "import duckdb\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b84b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_sites = [\n",
    "    \"09152500\",   # Gunnison River Near Grand Junction, CO\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c43794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "sites = usgs_sites\n",
    "parameter_codes = ['00060', '00010']\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2022-03-01'\n",
    "service_code = 'iv'\n",
    "output_root = 'data/data_lake'\n",
    "# ======================\n",
    "\n",
    "\n",
    "# Configure logging ------------------------------------------------\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "log_name = 'logs/' + dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.log'\n",
    "logging.basicConfig(filename=log_name,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e66295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: 09152500\n"
     ]
    }
   ],
   "source": [
    "# Itterate through each site main function\n",
    "for site in sites:\n",
    "    print(f\"Processing site: {site}\")\n",
    "    all_data = []\n",
    "\n",
    "    # Itterate through each parameter code\n",
    "    for pcode in parameter_codes:\n",
    "        \n",
    "        # Fetch data for the current site and parameter code from NWIS\n",
    "        df_raw = wdu.fetch_data(site, pcode, start_date, end_date, service_code)\n",
    "        if df_raw is None:\n",
    "            continue\n",
    "\n",
    "        # Clean and transform the data, standardizing column names and types\n",
    "        df_transformed = wdu.transform_data(df_raw, site, pcode)\n",
    "        if df_transformed.empty:\n",
    "            continue\n",
    "\n",
    "        # Write the data to a parquet file\n",
    "        wdu.write_to_datalake(df_transformed, site, output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d73f2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       site            datetime parameter  value approval_status  year\n",
      "0  09152500 2022-01-01 07:00:00     00010    2.4               A  2022\n",
      "1  09152500 2022-01-01 07:15:00     00010    2.4               A  2022\n",
      "2  09152500 2022-01-01 07:30:00     00010    2.4               A  2022\n",
      "3  09152500 2022-01-01 07:45:00     00010    2.4               A  2022\n",
      "4  09152500 2022-01-01 08:00:00     00010    2.4               A  2022\n",
      "5  09152500 2022-01-01 08:15:00     00010    2.4               A  2022\n",
      "6  09152500 2022-01-01 08:30:00     00010    2.4               A  2022\n",
      "7  09152500 2022-01-01 08:45:00     00010    2.4               A  2022\n",
      "8  09152500 2022-01-01 09:00:00     00010    2.4               A  2022\n",
      "9  09152500 2022-01-01 09:15:00     00010    2.3               A  2022\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM 'data/data_lake/timeseries_iv/site=09152500/year=2022/data.parquet'\"\n",
    "    \"LIMIT 10\"\n",
    ")\n",
    "# Connect to DuckDB and execute the query\n",
    "with duckdb.connect() as con:\n",
    "    result = con.execute(query).fetchdf()\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e1c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.read_parquet('data/data_lake/timeseries_iv/site=09152500/year=2022/data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0745e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────┬─────────────────────┬───────────┬────────┬─────────────────┬───────┐\n",
      "│   site   │      datetime       │ parameter │ value  │ approval_status │ year  │\n",
      "│ varchar  │    timestamp_ns     │  varchar  │ double │     varchar     │ int64 │\n",
      "├──────────┼─────────────────────┼───────────┼────────┼─────────────────┼───────┤\n",
      "│ 09152500 │ 2022-01-01 07:00:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 07:15:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 07:30:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 07:45:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 08:00:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 08:15:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 08:30:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 08:45:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 09:00:00 │ 00010     │    2.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-01-01 09:15:00 │ 00010     │    2.3 │ A               │  2022 │\n",
      "│    ·     │          ·          │   ·       │     ·  │ ·               │    ·  │\n",
      "│    ·     │          ·          │   ·       │     ·  │ ·               │    ·  │\n",
      "│    ·     │          ·          │   ·       │     ·  │ ·               │    ·  │\n",
      "│ 09152500 │ 2022-03-02 04:30:00 │ 00010     │    6.8 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 04:45:00 │ 00010     │    6.7 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 05:00:00 │ 00010     │    6.7 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 05:15:00 │ 00010     │    6.6 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 05:30:00 │ 00010     │    6.5 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 05:45:00 │ 00010     │    6.5 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 06:00:00 │ 00010     │    6.4 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 06:15:00 │ 00010     │    6.3 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 06:30:00 │ 00010     │    6.3 │ A               │  2022 │\n",
      "│ 09152500 │ 2022-03-02 06:45:00 │ 00010     │    6.2 │ A               │  2022 │\n",
      "├──────────┴─────────────────────┴───────────┴────────┴─────────────────┴───────┤\n",
      "│ 5759 rows (20 shown)                                                6 columns │\n",
      "└───────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a623c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Unable to merge: Field site has incompatible types: string vs dictionary<values=int32, indices=int32, ordered=0>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowTypeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m table = \u001b[43mpq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/data_lake/timeseries_iv/site=09152500/year=2022/data.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df = table.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1774\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1764\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1765\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1766\u001b[39m                memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m, partitioning=\u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1770\u001b[39m                thrift_container_size_limit=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1771\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1792\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1793\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1794\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1361\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1358\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1359\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:794\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    783\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    784\u001b[39m     schema=schema,\n\u001b[32m    785\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    791\u001b[39m )\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:486\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    478\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    479\u001b[39m     partitioning=partitioning,\n\u001b[32m    480\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    481\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    482\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    483\u001b[39m )\n\u001b[32m    484\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/_dataset.pyx:3198\u001b[39m, in \u001b[36mpyarrow._dataset.DatasetFactory.finish\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_projects/ucpo_waterdata/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowTypeError\u001b[39m: Unable to merge: Field site has incompatible types: string vs dictionary<values=int32, indices=int32, ordered=0>"
     ]
    }
   ],
   "source": [
    "table = pq.read_table('data/data_lake/timeseries_iv/site=09152500/year=2022/data.parquet')\n",
    "df = table.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
